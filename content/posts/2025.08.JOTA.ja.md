---
date: 2025-08-25
publishDate: 2025-08-25
title: Majorization-minimization Bregman proximal gradient algorithms for NMF with the Kullback--Leibler divergence
tags: ["Journal Paper"]
math: true
draft: false
---

Accepted for publication in JOTA.

### Authors:
- Shota Takahashi
- Mirai Tanaka
- Shiro Ikeda

### keywords:
- Nonnegative Matrix Factorization
- Bregman divergence
- proximal gradient algorithm

### URL:
- <a href="https://arxiv.org/abs/2405.11185" target="_blank" rel="noopener">arXiv Link</a>

---

### Abstract:

Nonnegative matrix factorization (NMF) is a popular method in machine learning and signal processing to decompose a given nonnegative matrix into two nonnegative matrices. In this paper, we propose new algorithms, called majorization-minimization Bregman proximal gradient algorithm (MMBPG) and MMBPG with extrapolation (MMBPGe) to solve NMF. These iterative algorithms minimize the objective function and its potential function monotonically. Assuming the Kurdyka--≈Åojasiewicz property, we establish that a sequence generated by MMBPG(e) globally converges to a stationary point. We apply MMBPG and MMBPGe to the Kullback--Leibler (KL) divergence-based NMF. While most existing KL-based NMF methods update two blocks or each variable alternately, our algorithms update all variables simultaneously. MMBPG and MMBPGe for KL-based NMF are equipped with a separable Bregman distance that satisfies the smooth adaptable property and that makes its subproblem solvable in closed form. Using this fact, we guarantee that a sequence generated by MMBPG(e) globally converges to a Karush--Kuhn--Tucker (KKT) point of KL-based NMF. In numerical experiments, we compare proposed algorithms with existing algorithms on synthetic data and real-world data.
